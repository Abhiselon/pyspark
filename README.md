# PySpark Tutorial for my Practice - Jupyter Notebooks

Welcome to the PySpark Tutorial GitHub repository!

Social Accounts: [Linkedin](https://www.linkedin.com/in/abhishek-kulkarni26/) and [GitHub](https://github.com/Abhiselon)
I post content on LinkedIn regularly too.

## Table of Contents

- [Introduction](#introduction)
- [Getting Started](#getting-started)
- [Notebook Descriptions](#notebook-descriptions)
- [Prerequisites](#prerequisites)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Introduction

These jupyter notebook contains some practice materials related to various topics like SparkContext, SparkSession, RDD transformations and actions, Spark DataFrames, Spark SQL, and more.

## Getting Started

To get started with the Jupyter notebooks, follow these steps:

1. Clone this GitHub repository to your local machine using the following command:

   ```bash
   git clone https://github.com/Abhiselon/pyspark.git
   ```

2. Ensure you have Python, Spark and Jupyter Notebook installed on your machine.

3. Launch Jupyter Notebook by running:

   ```bash
   jupyter notebook
   ```

4. Open the notebook you want to work on and start experimenting with PySpark.

## Notebook Descriptions

- **Notebook 1 - 01_Basic_Pyspark.ipynb**: Instructions and commands for setting the PySpark environment variables to use spark in jupyter notebook.

- **Notebook 2 - 02_Create_SparkContext.ipynb**: Creating SparkContext objects in different PySpark versions.

- **Notebook 3 - 03_Create_SparkSession.ipynb**: Creating SparkSession objects in PySpark.

- **Notebook 4 - 04_RDD_Operations.ipynb**: Creating RDD and Demonstrating RDD transformations and actions.

- **Notebook 5 - 05_Dataframes.ipynb**: Introduction to Spark DataFrames and differences compared to RDD.

- **Notebook 6 - 06_DataFrame_Operations.ipynb**: Performing Spark Dataframe operations like filtering, aggregation, etc.

- **Notebook 7 - 07_SparkSQL.ipynb**: Converting Spark Dataframe to a temporary table or view and performing SQL operations using Spark SQL.

Feel free to explore and run these notebooks at your own pace.

## Prerequisites

To make the most of these notebooks, you should have the following prerequisites:

- Basic knowledge of Python programming.

- Understanding of data processing concepts (though no prior PySpark experience is required).

## Usage

These notebooks are meant for self-learning and practice. Experiment with the code, modify it and try additional exercises to solidify your skills.

## Contributing

If you'd like to contribute to this repository by adding more notebooks, improving documentation, or fixing issues, please feel free to fork the repository, make your changes, and submit a pull request. We welcome contributions from the community!
